{
  "hash": "8a3d838eec61cfc4145a18bd9f66fa25",
  "result": {
    "markdown": "---\ntitle: 'The four \"C\"s for assessing data usefulness'\nauthor: \"Rebecca Barter\"\nformat:\n  html:\n    toc: true\n    toc-location: left\ncategories: [data science, data, data usefulness]\ndate: 2023-03-31\nimage: img/data_usefulness/four_c.jpg\ndescription: \"\"\ndraft: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n![](img/data_usefulness/four_c.jpg){fig-alt='A logo for the data usefulness'}\n\n# What is \"data usefulness\"?\n\nEvery data science project starts with a real-world question and a dataset that can (hopefully!) be used to find some insights that answer the question. But all datasets are not made equal, and the ability of your particular dataset to lead us to a reliable and trustworthy answer depends not just on your own analysis skills, but also on the *usefulness* of the data that you're working with. \n\n\"*Data usefulness*\" is a proxy for how well the data reflects the underlying reality it is supposed to represent as well as how easy it is to work with, and it should be evaluated in the context of the particular problem that you're trying to solve or question you're trying to answer. After all, if your data is not reflecting reality, or isn't relevant to the problem you're trying to solve, then it's not going to be of much use now, is it?\n\nExamples of data that has low usefulness is data whose values *don't accurately reflect* the quantities they are supposed to capture (e.g., measuring the temperature with a broken thermometer), data that is *missing relevant* variables or observations (e.g., a public opinion survey that was only taken by your followers on social media), data whose values are formatted *inconsistently* (e.g., state being recorded in inconsistent ways, such as \"California\", \"CA\", \"Cali\"), and data that is *outdated* (e.g., ). While the inconsistent formatting is often easy to fix, the other issues aren't.\n\nTo evaluate data quality, I use the **four \"C\"s** to assess data quality. The four \"C\"s are:\n\n1. **Correct**ness: Are the values in the data *correct*, i.e., are they accurately reflecting the real-world quantities they were designed to capture? \n\n1. **Complete**ness: Does the data contain all of the relevant information needed to answer your question?\n\n1. **Consist**ency: Was the data collected and formatted in a consistent way?\n\n1. **Currency**: Is the data up-to-date and relevant to the situation that you plan to apply it to?\n\nThese measures are more *qualitative* than they are *quantitative*, but can be really helpful for giving you a sense of your data's potential to provide insights for your real-world problem.\n\n\n# The four \"C\"s for assessing data quality\n\nBy way of an example, let's assess the usefulness of the CDC's NHANES survey data for answering the question of whether there is any relationship between inactivity and depression.\n\n\n\n## Correctness\n\nCorrectness is a measure of whether the values reported in the data are an accurate reflection of the real-world quantity they are designed to measure. \n\nFor example, one of the primary relevant variables in the NHANES data is the `PAD680` variable, which encodes the responses to the question of \"*How much time do you usually spend sitting on a typical day?*\" The answers to this question are reported in minutes and range from 0 to 1200, as well as options for \"don't know\" and \"refused\". Around 70% of respondents have a numeric value for this variable, and around 30% of respondents have a *missing value* for this variable (less than 1% have a \"don't know\" or \"refused\" response).\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell caption='A histogram showing the distribution of responses to the NHANES question \\'How much time do you usually spend sitting on a typical day?\\', with a ghost column added to convey the number of missing values.'}\n::: {.cell-output-display}\n![](2023-03-31-data_quality_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nHow well do you think this variable (`PAD680`) is reflecting reality? To answer this question, we need to first ask what is this variable trying to capture? The question being asked is \"How much time do you usually spend sitting on a typical day?\", so clearly this variable is trying to capture is the amount of time each person included in the dataset spends sitting in a typical day (note that the question specifically says that this should not include time sleeping).\n\nLet's start with the 30% of values that are missing. Do you think that these values are reflecting reality? They definitely are *not* reflecting reality: they are not at all capturing the amount of time each person spent sitting in a typical day.  \n\nPresumably these values are missing because the individual didn't have a good estimate of how much time they spent sitting on any given day, but this begs the question of why are these values are recorded as missing, rather than \"refused\" or \"don't know\"? Fewer than 1% of answers were \"refused\" or \"don't know\", which implies a potential inconsistency in the way that the data are being collected by the survey takers. We'll get to consistency below.\n\nOkay, so in this case the missing values aren't reflecting reality, and so are not considered \"correct\", but what about the non-missing values. Do you think that these values are \"correct\" in the sense that they are accurately reflecting the actual amount of time each person in the study spends sitting on a typical day? Since this data was not collected by *observing* how much each person sits on a typical day, and it's really hard to actually guess exactly how much time you spend sitting on a typical day (try it for yourself), these values are, at best, vague ball-park figures that are deeply influenced by flawed self-perception and a fear of being judged on their answer.\n\nIn this case, even the non-missing values are unlikely to be \"correct\". Don't lose all hope yet, however! This doesn't mean that you can't analyze your data, it just means that you need to be careful to remember and communicate that the results are based on self-reported measures of inactivity that are not perfectly accurate reflections of reality, which should serve as a reminder not to take any specific numeric conclusions that you arrive at too seriously (e.g., people who spend more than half of their waking hours sitting are more likely to be depressed). General trends, however, are likely to be . If you want to *test* the stability of your conclusions to your data, check out the [*Veridical Data Science* paper](https://www.pnas.org/doi/10.1073/pnas.1901326117) (And eventually Bin Yu and my veridical data science book) for advice on how to do that.\n\n\n\n\n## Completeness\n\nCompleteness is a measure \n\n## Consistency\n\n## Currency\n\n\n\n# Can data cleaning help?\n\nWhile data cleaning can address consistency (e.g., by reformatting inconsistently recorded data values) correctness to some extent (e.g., by replacing invalid or incorrect values with more correct values when they are known), data cleaning cannot fix a lack of completeness or currency.\n\n",
    "supporting": [
      "2023-03-31-data_quality_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}