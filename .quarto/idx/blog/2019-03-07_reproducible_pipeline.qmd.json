{"title":"A quick guide to developing a reproducible and consistent data science workflow","markdown":{"yaml":{"title":"A quick guide to developing a reproducible and consistent data science workflow","author":"Rebecca Barter","format":{"html":{"toc":true,"toc-location":"left"}},"categories":["data science","workflow","reproducibility"],"date":"2019-03-23","description":"When you're learning to code and perform data analysis, it can be overwhelming to figure out how to structure your projects. To help data scientists develop a reproducible and consistent workflow, I've put together a short document with some guiding advice."},"headingText":"Reproducbility workflow","containsRefs":false,"markdown":"\n\nWhen you're learning to code and perform data analysis, it can be overwhelming to figure out how to structure your projects. To help data scientists develop a reproducible and consistent workflow, I've put together a short GitHub-based document with some guiding advice: [https://github.com/rlbarter/reproducibility-workflow](https://github.com/rlbarter/reproducibility-workflow)\n\nIf you're interested in contributing or improving this document, please get in touch, or even better, submit a pull request [https://github.com/rlbarter/reproducibility-workflow](https://github.com/rlbarter/reproducibility-workflow)!\n\nThe document as of writing is shown below.\n\n\nEvery project should consist of a single well structured directory with meaningful subdirectories. Every project should be its own git repository that is hosted on GitHub.\n\nData cleaning and analyses should be carefully documented in a Jupyter Notebook or R markdown file and should be created with reproducibility in mind. Everyone on the team (and future you) should be able to re-create what you have performed.\n\nThe overall purpose is to have an organized project structure in place so that the project is easily approachable to many different individuals.\n\n> Someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why - Bill Noble\n\nAn example of a reproducible project that follows this workflow lives in the [project_example/](https://github.com/rlbarter/reproducibility-workflow/tree/master/project_example) folder.\n\n### Project Structure\n\nThe project will typically consist of the following subdirectories:\n\n#### Data\n\nOriginal raw data files data should be backed up on something like Google Drive, Dropbox or Box. The raw data itself should never be touched manually. Instead, you should have scripts or notebooks that load the raw data into an R or Python environment for in-environment data manipulation (this will not modify the raw data files themselves).\n\nAny data that is produced by code should be saved in the `data/processed_data/` subdirectory.\n\n#### Documents\n\nThis is a good place to keep meeting notes, data dictionaries, and any other associated materials.\n\n\n#### Code\n\nThere are three types of code documents:\n\n1. **Function scripts** (.R, .py): scripts that contain reusable functions that will be called in the action scripts below (and possibly in the exploration notebooks). By convention, function scripts are given the name `xx_funs_yy.R`, where `xx` is a number and `yy` describes what the functions are for (e.g. `01_funs_clean_data.R`).\n\n1. **Action scripts** (.R, .py): scripts that perform activities such as a detailed data cleaning pipeline, or running many models. Often these scripts will load in data, do something to it (e.g. clean it or fit a model to it) and will then save a new object (such as a cleaned dataset or model results). By convention, action scripts are given the name `xx_do_yy.R`, where `xx` is a number and `yy` describes what action is undertaken by running the script (e.g. `01_do_clean_data.R`).\n\n1. **Exploration notebooks** (.Rmd, .ipynb): R Markdown or Jupyter notebook files that are used to produce figures and explanatory files that contain figures and explanations of data cleaning steps and results of analyses. These are the files that an external viewer would find useful to understand your data and analysis.\n\nScripts that are run sequentially are numbered accordingly. An example of a project structure is shown below. Note that in the example below the functions folder is nested as a subdirectory of the scripts folder which contains the actionable scripts. This makes sense when the functions are only called in the actionable scripts (but not in the exploration notebooks).\n\n\n```\nproject\n│   README.md\n└───data/\n│       └───raw_data/\n│           │   data_orig.csv\n│       └───processed_data/\n│           │   data_clean.csv\n│       └───results/\n│           │   model_results.csv\n└───documents/\n│       meeting_notes.md\n│       data_dictionary.md\n└───code/\n│       └───exploration/\n│           │   01_data_exploration.Rmd\n│           │   02_model_results.Rmd\n│       └───scripts/\n│           │   01_do_clean_data.R\n│           │   02_do_model_data.R\n│           └───functions/\n│               │   01_funs_clean_data.R\n│               │   02_funs_model_data.R\n\n```\n\n### Syntax and conventions\n\nAll filenames are always lowercase and use underscores to separate words.\n\nCode should follow an appropriate style guide:\n\n- R: [Tidyverse Style Guide](https://style.tidyverse.org/) (based on the R Google Style Guide)\n- Python: [Google Style Guide](https://google.github.io/styleguide/pyguide.html)\n\n\n### Resources\n\n- I wrote a much more detailed blog post on my workflow a few years ago that can be found here: [http://www.rebeccabarter.com/blog/2017-08-16-data-science-workflow/](http://www.rebeccabarter.com/blog/2017-08-16-data-science-workflow/). My workflow has changed a bit since then, but the underlying ideas are all more or less the same.\n\n- William Noble's article on organizing computational biology projects: [https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424)\n\n- Marwick, Boettiger and Mullen's article on packaging data analytical work reproducibly: [https://ro.uow.edu.au/cgi/viewcontent.cgi?article=6445&context=smhpapers](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=6445&context=smhpapers)\n\n### Acknowledgements\n\nThanks very much to Ciera Martinez for sharing her [project workflow](https://github.com/DiscoveryDNA/team_neural_network/blob/master/data_managment.md).\n\nI'd also like to acknowledge the *Meta Research and Best Practices* working group (formerly the *Reproducility working group*) at the Berkeley Institute for Data Science (BIDS) for insightful discussions that have helped me form my own workflow over the years.\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"2019-03-07_reproducible_pipeline.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":"flatly","title-block-banner":true,"title":"A quick guide to developing a reproducible and consistent data science workflow","author":"Rebecca Barter","categories":["data science","workflow","reproducibility"],"date":"2019-03-23","description":"When you're learning to code and perform data analysis, it can be overwhelming to figure out how to structure your projects. To help data scientists develop a reproducible and consistent workflow, I've put together a short document with some guiding advice.","toc-location":"left"},"extensions":{"book":{"multiFile":true}}}}}